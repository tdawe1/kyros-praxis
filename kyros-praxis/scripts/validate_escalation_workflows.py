#!/usr/bin/env python3
"""
Workflow Validation Scripts for Hybrid Model Escalation System

This module provides comprehensive validation scripts for testing escalation workflows,
including automated validation, manual testing procedures, and integration validation.
"""

import json
import yaml
import subprocess
import time
import asyncio
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path
from dataclasses import dataclass
from enum import Enum
import logging
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow_validation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ValidationStatus(Enum):
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    WARNING = "warning"

@dataclass
class ValidationResult:
    """Result of a validation check."""
    check_name: str
    status: ValidationStatus
    message: str
    details: Dict[str, Any] = None
    execution_time: float = 0.0

@dataclass
class EscalationTestResult:
    """Result of an escalation workflow test."""
    scenario_name: str
    role: str
    expected_escalation: bool
    actual_escalation: bool
    model_used: str
    execution_time: float
    validation_results: List[ValidationResult]
    overall_status: ValidationStatus

class EscalationWorkflowValidator:
    """Main validator for escalation workflows."""
    
    def __init__(self, config_path: Optional[Path] = None):
        self.config = self._load_config(config_path)
        self.test_results: List[EscalationTestResult] = []
        self.validation_functions: Dict[str, Callable] = {
            'config_validation': self._validate_configuration,
            'trigger_validation': self._validate_trigger_logic,
            'model_selection': self._validate_model_selection,
            'cost_tracking': self._validate_cost_tracking,
            'performance_metrics': self._validate_performance_metrics,
            'integration_testing': self._validate_integration
        }
    
    def _load_config(self, config_path: Optional[Path]) -> Dict[str, Any]:
        """Load validation configuration from file."""
        default_config = {
            'test_timeout': 300,  # seconds
            'performance_thresholds': {
                'max_response_time': 60.0,
                'min_accuracy': 0.95,
                'max_cost_threshold': 0.10
            },
            'validation_levels': ['basic', 'comprehensive', 'stress'],
            'log_level': 'INFO'
        }
        
        if config_path and config_path.exists():
            try:
                with open(config_path, 'r') as f:
                    if config_path.suffix.lower() == '.yaml':
                        loaded_config = yaml.safe_load(f)
                    else:
                        loaded_config = json.load(f)
                default_config.update(loaded_config)
            except Exception as e:
                logger.warning(f"Failed to load config from {config_path}: {e}")
        
        return default_config
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run all validation checks and return comprehensive report."""
        logger.info("üöÄ Starting comprehensive workflow validation")
        start_time = time.time()
        
        report = {
            'validation_start': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_tests': 0,
            'passed': 0,
            'failed': 0,
            'warnings': 0,
            'skipped': 0,
            'test_results': [],
            'performance_summary': {},
            'recommendations': []
        }
        
        # Run all validation functions
        for validation_name, validation_func in self.validation_functions.items():
            logger.info(f"üîç Running {validation_name} validation...")
            try:
                result = validation_func()
                if isinstance(result, list):
                    report['test_results'].extend(result)
                else:
                    report['test_results'].append(result)
            except Exception as e:
                logger.error(f"‚ùå {validation_name} validation failed: {e}")
                report['test_results'].append(ValidationResult(
                    check_name=validation_name,
                    status=ValidationStatus.FAILED,
                    message=f"Validation failed with exception: {str(e)}"
                ))
        
        # Compile summary statistics
        for result in report['test_results']:
            report['total_tests'] += 1
            if result.status == ValidationStatus.PASSED:
                report['passed'] += 1
            elif result.status == ValidationStatus.FAILED:
                report['failed'] += 1
            elif result.status == ValidationStatus.WARNING:
                report['warnings'] += 1
            elif result.status == ValidationStatus.SKIPPED:
                report['skipped'] += 1
        
        # Generate recommendations
        report['recommendations'] = self._generate_recommendations(report)
        
        # Add performance summary
        report['performance_summary'] = {
            'total_execution_time': time.time() - start_time,
            'average_execution_time': sum(r.execution_time for r in report['test_results']) / len(report['test_results']) if report['test_results'] else 0
        }
        
        report['validation_end'] = time.strftime('%Y-%m-%d %H:%M:%S')
        
        logger.info(f"‚úÖ Validation completed. Results: {report}")
        return report
    
    def _validate_configuration(self) -> ValidationResult:
        """Validate hybrid model configuration files."""
        logger.info("üîß Validating configuration files...")
        start_time = time.time()
        
        config_files = [
            Path('.claude/custom_modes.yml'),
            Path('.claude/model-config.json'),
            Path('modes.yml')
        ]
        
        issues = []
        validations_passed = 0
        
        for config_file in config_files:
            if not config_file.exists():
                issues.append(f"Missing config file: {config_file}")
                continue
            
            try:
                with open(config_file, 'r') as f:
                    if config_file.suffix.lower() == '.yml':
                        config = yaml.safe_load(f)
                    else:
                        config = json.load(f)
                
                # Validate escalation configuration
                if 'customModes' in config:
                    for mode in config['customModes']:
                        if 'customInstructions' in mode:
                            instructions = mode['customInstructions']
                            if 'ESCALATE' in instructions:
                                validations_passed += 1
                                logger.info(f"‚úÖ Found escalation rules for {mode['slug']}")
                
                logger.info(f"‚úÖ Config file {config_file} is valid")
                validations_passed += 1
                
            except Exception as e:
                issues.append(f"Invalid config file {config_file}: {str(e)}")
        
        status = ValidationStatus.PASSED if not issues else ValidationStatus.FAILED
        message = f"Configuration validation: {len(validations_passed)} checks passed" if not issues else f"Configuration issues found: {len(issues)}"
        
        return ValidationResult(
            check_name="configuration_validation",
            status=status,
            message=message,
            details={
                "files_checked": len(config_files),
                "validations_passed": validations_passed,
                "issues": issues
            },
            execution_time=time.time() - start_time
        )
    
    def _validate_trigger_logic(self) -> List[ValidationResult]:
        """Validate escalation trigger logic for all roles."""
        logger.info("üéØ Validating escalation trigger logic...")
        results = []
        
        # Test Architect escalation triggers
        architect_triggers = [
            ("3+ service decision", True),
            ("Security implications", True),
            ("Performance-critical", True),
            ("Single service decision", False),
            ("Low risk changes", False)
        ]
        
        for trigger_name, should_escalate in architect_triggers:
            result = self._test_single_trigger("architect", trigger_name, should_escalate)
            results.append(result)
        
        # Test Integrator escalation triggers
        integrator_triggers = [
            ("Multi-service conflict", True),
            ("System boundary changes", True),
            ("Simple formatting", False)
        ]
        
        for trigger_name, should_escalate in integrator_triggers:
            result = self._test_single_trigger("integrator", trigger_name, should_escalate)
            results.append(result)
        
        return results
    
    def _test_single_trigger(self, role: str, trigger_name: str, should_escalate: bool) -> ValidationResult:
        """Test a single escalation trigger."""
        start_time = time.time()
        
        # Simulate trigger evaluation
        # In a real implementation, this would call the actual escalation logic
        actual_escalation = self._simulate_trigger_evaluation(role, trigger_name)
        
        status = ValidationStatus.PASSED if actual_escalation == should_escalate else ValidationStatus.FAILED
        message = f"{role} {trigger_name}: {'‚úÖ Correct' if status == ValidationStatus.PASSED else '‚ùå Failed'}"
        
        return ValidationResult(
            check_name=f"{role}_{trigger_name.replace(' ', '_')}_trigger",
            status=status,
            message=message,
            details={
                "role": role,
                "trigger": trigger_name,
                "expected_escalation": should_escalate,
                "actual_escalation": actual_escalation
            },
            execution_time=time.time() - start_time
        )
    
    def _simulate_trigger_evaluation(self, role: str, trigger_name: str) -> bool:
        """Simulate trigger evaluation logic."""
        # This is a simplified simulation
        # In a real implementation, this would evaluate actual trigger conditions
        
        architect_escalation_triggers = [
            "3+ service decision",
            "Security implications", 
            "Performance-critical"
        ]
        
        integrator_escalation_triggers = [
            "Multi-service conflict",
            "System boundary changes"
        ]
        
        if role == "architect":
            return trigger_name in architect_escalation_triggers
        elif role == "integrator":
            return trigger_name in integrator_escalation_triggers
        
        return False
    
    def _validate_model_selection(self) -> ValidationResult:
        """Validate model selection logic for escalation scenarios."""
        logger.info("ü§ñ Validating model selection logic...")
        start_time = time.time()
        
        test_cases = [
            ("architect", True, "claude-4.1-opus"),
            ("architect", False, "glm-4.5"),
            ("integrator", True, "claude-4.1-opus"),
            ("integrator", False, "glm-4.5"),
            ("orchestrator", False, "glm-4.5"),
            ("implementer", False, "glm-4.5"),
            ("critic", False, "glm-4.5")
        ]
        
        passed = 0
        failed = 0
        
        for role, should_escalate, expected_model in test_cases:
            actual_model = self._simulate_model_selection(role, should_escalate)
            if actual_model == expected_model:
                passed += 1
            else:
                failed += 1
                logger.warning(f"‚ùå Model selection failed: {role} escalate={should_escalate}, expected={expected_model}, got={actual_model}")
        
        status = ValidationStatus.PASSED if failed == 0 else ValidationStatus.FAILED
        message = f"Model selection validation: {passed} passed, {failed} failed"
        
        return ValidationResult(
            check_name="model_selection_validation",
            status=status,
            message=message,
            details={
                "test_cases": len(test_cases),
                "passed": passed,
                "failed": failed
            },
            execution_time=time.time() - start_time
        )
    
    def _simulate_model_selection(self, role: str, should_escalate: bool) -> str:
        """Simulate model selection logic."""
        if should_escalate and role in ["architect", "integrator"]:
            return "claude-4.1-opus"
        return "glm-4.5"
    
    def _validate_cost_tracking(self) -> ValidationResult:
        """Validate cost tracking functionality."""
        logger.info("üí∞ Validating cost tracking...")
        start_time = time.time()
        
        try:
            # Test if monitoring script exists and is executable
            monitor_script = Path("scripts/model-usage-monitor.py")
            if not monitor_script.exists():
                return ValidationResult(
                    check_name="cost_tracking_validation",
                    status=ValidationStatus.FAILED,
                    message="Cost tracking script not found",
                    execution_time=time.time() - start_time
                )
            
            # Run the monitoring script
            result = subprocess.run(
                ["python", str(monitor_script), "report"],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                return ValidationResult(
                    check_name="cost_tracking_validation",
                    status=ValidationStatus.PASSED,
                    message="Cost tracking script executed successfully",
                    details={
                        "output": result.stdout,
                        "script_path": str(monitor_script)
                    },
                    execution_time=time.time() - start_time
                )
            else:
                return ValidationResult(
                    check_name="cost_tracking_validation",
                    status=ValidationStatus.FAILED,
                    message=f"Cost tracking script failed: {result.stderr}",
                    execution_time=time.time() - start_time
                )
                
        except subprocess.TimeoutExpired:
            return ValidationResult(
                check_name="cost_tracking_validation",
                status=ValidationStatus.FAILED,
                message="Cost tracking script timed out",
                execution_time=time.time() - start_time
            )
        except Exception as e:
            return ValidationResult(
                check_name="cost_tracking_validation",
                status=ValidationStatus.FAILED,
                message=f"Cost tracking validation failed: {str(e)}",
                execution_time=time.time() - start_time
            )
    
    def _validate_performance_metrics(self) -> ValidationResult:
        """Validate performance metrics collection."""
        logger.info("‚ö° Validating performance metrics...")
        start_time = time.time()
        
        # Test response times for different scenarios
        scenarios = [
            ("simple_task", "glm-4.5", 10.0),
            ("complex_task", "claude-4.1-opus", 30.0),
            ("escalation_decision", "claude-4.1-opus", 15.0)
        ]
        
        performance_results = []
        
        for scenario, model, max_time in scenarios:
            simulated_time = self._simulate_response_time(scenario, model)
            within_threshold = simulated_time <= max_time
            
            performance_results.append({
                "scenario": scenario,
                "model": model,
                "simulated_time": simulated_time,
                "max_allowed": max_time,
                "within_threshold": within_threshold
            })
        
        passed = sum(1 for r in performance_results if r["within_threshold"])
        total = len(performance_results)
        
        status = ValidationStatus.PASSED if passed == total else ValidationStatus.WARNING
        message = f"Performance validation: {passed}/{total} scenarios within threshold"
        
        return ValidationResult(
            check_name="performance_metrics_validation",
            status=status,
            message=message,
            details={
                "performance_results": performance_results,
                "thresholds": self.config['performance_thresholds']
            },
            execution_time=time.time() - start_time
        )
    
    def _simulate_response_time(self, scenario: str, model: str) -> float:
        """Simulate response time for a scenario."""
        # This is a simplified simulation
        # In a real implementation, this would measure actual response times
        
        base_times = {
            "glm-4.5": 5.0,
            "claude-4.1-opus": 20.0
        }
        
        scenario_multipliers = {
            "simple_task": 1.0,
            "complex_task": 2.0,
            "escalation_decision": 1.5
        }
        
        return base_times.get(model, 10.0) * scenario_multipliers.get(scenario, 1.0)
    
    def _validate_integration(self) -> ValidationResult:
        """Validate end-to-end integration of escalation workflows."""
        logger.info("üîó Validating integration...")
        start_time = time.time()
        
        # Test integration scenarios
        integration_tests = [
            "config_loading",
            "trigger_evaluation", 
            "model_selection",
            "cost_tracking",
            "performance_monitoring"
        ]
        
        passed = 0
        failed = 0
        errors = []
        
        for test_name in integration_tests:
            try:
                # Simulate integration test
                success = self._simulate_integration_test(test_name)
                if success:
                    passed += 1
                else:
                    failed += 1
                    errors.append(f"{test_name} failed")
            except Exception as e:
                failed += 1
                errors.append(f"{test_name} exception: {str(e)}")
        
        status = ValidationStatus.PASSED if failed == 0 else ValidationStatus.FAILED
        message = f"Integration validation: {passed} passed, {failed} failed"
        
        return ValidationResult(
            check_name="integration_validation",
            status=status,
            message=message,
            details={
                "tests_run": len(integration_tests),
                "passed": passed,
                "failed": failed,
                "errors": errors
            },
            execution_time=time.time() - start_time
        )
    
    def _simulate_integration_test(self, test_name: str) -> bool:
        """Simulate an integration test."""
        # This would be replaced with actual integration tests
        integration_success_rates = {
            "config_loading": 0.95,
            "trigger_evaluation": 0.90,
            "model_selection": 0.98,
            "cost_tracking": 0.85,
            "performance_monitoring": 0.80
        }
        
        import random
        return random.random() < integration_success_rates.get(test_name, 0.90)
    
    def _generate_recommendations(self, report: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on validation results."""
        recommendations = []
        
        # Check failure rates
        if report['failed'] > 0:
            failure_rate = report['failed'] / report['total_tests']
            recommendations.append(f"High failure rate detected ({failure_rate:.1%}). Review failed tests and address underlying issues.")
        
        # Check performance
        if 'performance_summary' in report:
            avg_time = report['performance_summary'].get('average_execution_time', 0)
            if avg_time > 30:
                recommendations.append(f"Slow average execution time ({avg_time:.1f}s). Consider optimization.")
        
        # Check specific validation areas
        for result in report['test_results']:
            if result.status == ValidationStatus.FAILED:
                if "configuration" in result.check_name:
                    recommendations.append("Review and fix configuration files.")
                elif "model_selection" in result.check_name:
                    recommendations.append("Fix model selection logic in escalation workflows.")
                elif "cost_tracking" in result.check_name:
                    recommendations.append("Ensure cost tracking script is properly configured.")
        
        # General recommendations
        recommendations.extend([
            "Implement automated monitoring of escalation workflows.",
            "Set up alerts for escalation failures or performance degradation.",
            "Regularly review escalation criteria and adjust as needed."
        ])
        
        return recommendations
    
    def save_report(self, report: Dict[str, Any], output_path: Path):
        """Save validation report to file."""
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        logger.info(f"üíæ Validation report saved to {output_path}")

def main():
    """Main execution function."""
    validator = EscalationWorkflowValidator()
    
    print("üöÄ Starting Hybrid Model Escalation Workflow Validation")
    print("=" * 60)
    
    # Run comprehensive validation
    report = validator.run_comprehensive_validation()
    
    # Save report
    report_path = Path("validation_report.json")
    validator.save_report(report, report_path)
    
    # Print summary
    print(f"\nüìä Validation Summary:")
    print(f"  Total Tests: {report['total_tests']}")
    print(f"  ‚úÖ Passed: {report['passed']}")
    print(f"  ‚ùå Failed: {report['failed']}")
    print(f"  ‚ö†Ô∏è  Warnings: {report['warnings']}")
    print(f"  ‚è≠Ô∏è  Skipped: {report['skipped']}")
    
    print(f"\n‚è±Ô∏è  Performance:")
    print(f"  Total Time: {report['performance_summary']['total_execution_time']:.1f}s")
    print(f"  Average Test Time: {report['performance_summary']['average_execution_time']:.2f}s")
    
    if report['recommendations']:
        print(f"\nüí° Recommendations:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"  {i}. {rec}")
    
    print(f"\nüìÑ Full report saved to: {report_path}")
    
    # Exit with appropriate code
    return 0 if report['failed'] == 0 else 1

if __name__ == "__main__":
    exit(main())